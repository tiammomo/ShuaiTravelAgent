"""
================================================================================
ReAct æ—…æ¸¸åŠ©æ‰‹ Agent - æ ¸å¿ƒå®ç°æ¨¡å—
================================================================================

æœ¬æ¨¡å—å®ç°äº†åŸºäº ReAct (Reasoning and Acting) æ¨¡å¼çš„æ—…æ¸¸æ™ºèƒ½ä½“ã€‚

åŠŸèƒ½æ¦‚è¿°ï¼š
- æä¾›å®Œæ•´çš„æ—…æ¸¸ç›¸å…³å·¥å…·é›†ï¼ˆåŸå¸‚æœç´¢ã€æ™¯ç‚¹æŸ¥è¯¢ã€è·¯çº¿è§„åˆ’ã€é¢„ç®—è®¡ç®—ç­‰ï¼‰
- é›†æˆ LLM è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£å’Œå›ç­”ç”Ÿæˆ
- æ”¯æŒåŒæ­¥å’Œæµå¼ä¸¤ç§å¤„ç†æ¨¡å¼
- ç»´æŠ¤å¯¹è¯å†å²å’Œç”¨æˆ·åå¥½

ReAct æ¨¡å¼æµç¨‹ï¼š
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œåˆ†ææ„å›¾
2. é€‰æ‹©åˆé€‚çš„å·¥å…·æ‰§è¡Œ
3. æ”¶é›†å·¥å…·æ‰§è¡Œç»“æœ
4. ä½¿ç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”

æ ¸å¿ƒç»„ä»¶ï¼š
- create_travel_tools: æ—…æ¸¸å·¥å…·å·¥å‚å‡½æ•°
- å·¥å…·æ‰§è¡Œå‡½æ•°: _search_cities, _query_attractions, _generate_route ç­‰
- ReActTravelAgent: æ—…æ¸¸åŠ©æ‰‹ä¸»ç±»

ä½¿ç”¨ç¤ºä¾‹ï¼š
```python
agent = ReActTravelAgent(config_path="config/llm_config.yaml")
result = await agent.process("åŒ—äº¬ä¸‰æ—¥æ¸¸æ¨è")
```

================================================================================
"""

import json
import sys
import os
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥æ”¯æŒå¤–éƒ¨å¯¼å…¥
# è¿™è§£å†³äº†æ¨¡å—é—´ç›¸å¯¹å¯¼å…¥çš„é—®é¢˜ï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®æ‰¾åˆ° coreã€config ç­‰æ¨¡å—
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
AGENT_SRC_DIR = os.path.dirname(CURRENT_DIR)
if AGENT_SRC_DIR not in sys.path:
    sys.path.insert(0, AGENT_SRC_DIR)

# ä½¿ç”¨ç»å¯¹å¯¼å…¥æ›¿ä»£ç›¸å¯¹å¯¼å…¥ï¼Œæé«˜ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
from core.react_agent import ReActAgent, ToolInfo, Action, Thought, AgentState, ActionStatus
from core.style_config import style_manager, ReplyStyle, StyleConfig
from core.intent_recognizer import intent_recognizer, IntentRecognizer, IntentResult, IntentType, SentimentType
from core.decision_engine import decision_engine, DecisionEngine, Decision, DecisionType, ContextInfo
from config.config_manager import ConfigManager
from memory.manager import MemoryManager
from llm.client import LLMClient
from enum import Enum


class ChatMode(Enum):
    """å¯¹è¯æ¨¡å¼æšä¸¾"""
    DIRECT = "direct"       # ç›´æ¥è°ƒç”¨ LLM
    REACT = "react"         # ReAct æ¨ç†æ¨¡å¼
    PLAN = "plan"           # è§„åˆ’åæ‰§è¡Œæ¨¡å¼


def create_travel_tools(config_manager: ConfigManager) -> List[tuple]:
    """
    åˆ›å»ºæ—…æ¸¸åŠ©æ‰‹å·¥å…·åˆ—è¡¨

    è¯¥å‡½æ•°æ˜¯æ—…æ¸¸å·¥å…·çš„å·¥å‚æ–¹æ³•ï¼Œè´Ÿè´£åˆ›å»ºæ‰€æœ‰å¯ç”¨çš„æ—…æ¸¸ç›¸å…³å·¥å…·ã€‚
    æ¯ä¸ªå·¥å…·ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š
    1. ToolInfo: å·¥å…·çš„å…ƒæ•°æ®æè¿°ï¼ˆåç§°ã€å‚æ•°ã€åˆ†ç±»ç­‰ï¼‰
    2. executor: å·¥å…·çš„å®é™…æ‰§è¡Œå‡½æ•°

    å·¥å…·åˆ—è¡¨åŒ…æ‹¬ï¼š
    - search_cities: æ ¹æ®æ¡ä»¶æœç´¢åŒ¹é…çš„åŸå¸‚
    - query_attractions: æŸ¥è¯¢åŸå¸‚æ™¯ç‚¹ä¿¡æ¯
    - generate_route: ç”Ÿæˆæ—…æ¸¸è·¯çº¿è§„åˆ’
    - calculate_budget: è®¡ç®—æ—…æ¸¸é¢„ç®—
    - get_city_info: è·å–åŸå¸‚è¯¦ç»†ä¿¡æ¯
    - llm_chat: LLM å¯¹è¯å›ç­”
    - generate_city_recommendation: ç”ŸæˆåŸå¸‚æ¨è
    - generate_route_plan: ç”Ÿæˆè¯¦ç»†è·¯çº¿è®¡åˆ’

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹ï¼Œç”¨äºè·å–åŸå¸‚æ•°æ®ç­‰ä¿¡æ¯

    Returns:
        List[tuple]: å·¥å…·å…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (ToolInfo, executor_func)

    Examples:
        >>> tools = create_travel_tools(config_manager)
        >>> for tool_info, executor in tools:
        ...     agent.register_tool(tool_info, executor)
    """
    from environment.travel_data import TravelData

    tools = []

    # ========== å·¥å…·1: åŸå¸‚æœç´¢ ==========
    # æ ¹æ®ç”¨æˆ·å…´è¶£ã€é¢„ç®—å’Œå­£èŠ‚åå¥½æœç´¢åŒ¹é…çš„åŸå¸‚
    tools.append((
        ToolInfo(
            name="search_cities",
            description="æ ¹æ®ç”¨æˆ·å…´è¶£ã€é¢„ç®—å’Œå­£èŠ‚åå¥½æœç´¢åŒ¹é…çš„åŸå¸‚",
            parameters={
                'type': 'object',
                'properties': {
                    'interests': {
                        'type': 'array',
                        'items': {'type': 'string'},
                        'description': 'ç”¨æˆ·å…´è¶£æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ ["ç¾é£Ÿ", "å†å²", "è‡ªç„¶é£å…‰"]'
                    },
                    'budget_min': {'type': 'integer', 'description': 'æœ€ä½é¢„ç®—é‡‘é¢ï¼ˆå…ƒï¼‰'},
                    'budget_max': {'type': 'integer', 'description': 'æœ€é«˜é¢„ç®—é‡‘é¢ï¼ˆå…ƒï¼‰'},
                    'season': {'type': 'string', 'description': 'æ—…è¡Œå­£èŠ‚ï¼Œå¦‚ "æ˜¥å­£", "å¤å­£"'}
                }
            },
            required_params=[],  # æ‰€æœ‰å‚æ•°éƒ½æ˜¯å¯é€‰çš„
            category='travel',
            tags=['search', 'city', 'recommend']
        ),
        # æ‰§è¡Œå‡½æ•°ï¼šè°ƒç”¨å†…éƒ¨å‡½æ•°å¤„ç†æœç´¢é€»è¾‘
        lambda interests=None, budget_min=None, budget_max=None, season=None:
            _search_cities(config_manager, interests, (budget_min, budget_max) if budget_min and budget_max else None, season)
    ))

    # ========== å·¥å…·2: æ™¯ç‚¹æŸ¥è¯¢ ==========
    # æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„æ™¯ç‚¹ä¿¡æ¯
    tools.append((
        ToolInfo(
            name="query_attractions",
            description="æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„æ™¯ç‚¹ä¿¡æ¯",
            parameters={
                'type': 'object',
                'properties': {
                    'cities': {
                        'type': 'array',
                        'items': {'type': 'string'},
                        'description': 'è¦æŸ¥è¯¢çš„åŸå¸‚åç§°åˆ—è¡¨'
                    }
                },
                'required': ['cities']  # cities æ˜¯å¿…å¡«å‚æ•°
            },
            required_params=['cities'],
            category='travel',
            tags=['query', 'attraction', 'scenic']
        ),
        lambda cities: _query_attractions(config_manager, cities)
    ))

    # ========== å·¥å…·3: è·¯çº¿ç”Ÿæˆ ==========
    # ä¸ºæŒ‡å®šåŸå¸‚ç”Ÿæˆè¯¦ç»†çš„æ—…æ¸¸è·¯çº¿è§„åˆ’
    tools.append((
        ToolInfo(
            name="generate_route",
            description="ä¸ºæŒ‡å®šåŸå¸‚ç”Ÿæˆè¯¦ç»†çš„æ—…æ¸¸è·¯çº¿è§„åˆ’",
            parameters={
                'type': 'object',
                'properties': {
                    'city': {'type': 'string', 'description': 'ç›®æ ‡åŸå¸‚åç§°'},
                    'days': {'type': 'integer', 'description': 'æ—…è¡Œå¤©æ•°ï¼Œé»˜è®¤3å¤©', 'default': 3}
                },
                'required': ['city']  # city æ˜¯å¿…å¡«å‚æ•°
            },
            required_params=['city'],
            category='travel',
            tags=['route', 'plan', 'schedule']
        ),
        lambda city, days=3: _generate_route(config_manager, city, days)
    ))

    # ========== å·¥å…·4: é¢„ç®—è®¡ç®— ==========
    # è®¡ç®—æŒ‡å®šåŸå¸‚å’Œå¤©æ•°çš„æ—…æ¸¸é¢„ç®—
    tools.append((
        ToolInfo(
            name="calculate_budget",
            description="è®¡ç®—æŒ‡å®šåŸå¸‚å’Œå¤©æ•°çš„æ—…æ¸¸é¢„ç®—",
            parameters={
                'type': 'object',
                'properties': {
                    'city': {'type': 'string', 'description': 'ç›®æ ‡åŸå¸‚'},
                    'days': {'type': 'integer', 'description': 'æ—…è¡Œå¤©æ•°'}
                },
                'required': ['city', 'days']  # city å’Œ days éƒ½æ˜¯å¿…å¡«å‚æ•°
            },
            required_params=['city', 'days'],
            category='travel',
            tags=['budget', 'cost', 'expense']
        ),
        lambda city, days: _calculate_budget(config_manager, city, days)
    ))

    # ========== å·¥å…·5: åŸå¸‚ä¿¡æ¯ ==========
    # è·å–æŒ‡å®šåŸå¸‚çš„è¯¦ç»†ä¿¡æ¯
    tools.append((
        ToolInfo(
            name="get_city_info",
            description="è·å–æŒ‡å®šåŸå¸‚çš„è¯¦ç»†ä¿¡æ¯",
            parameters={
                'type': 'object',
                'properties': {
                    'city': {'type': 'string', 'description': 'åŸå¸‚åç§°'}
                },
                'required': ['city']
            },
            required_params=['city'],
            category='travel',
            tags=['city', 'info', 'detail']
        ),
        lambda city: _get_city_info(config_manager, city)
    ))

    # ========== å·¥å…·6: LLM å¯¹è¯ ==========
    # ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯å›ç­”
    tools.append((
        ToolInfo(
            name="llm_chat",
            description="ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯å›ç­”",
            parameters={
                'type': 'object',
                'properties': {
                    'query': {'type': 'string', 'description': 'ç”¨æˆ·é—®é¢˜'},
                    'context': {'type': 'string', 'description': 'å¯¹è¯ä¸Šä¸‹æ–‡'}
                },
                'required': ['query']
            },
            required_params=['query'],
            category='ai',
            tags=['chat', 'llm', 'ai']
        ),
        lambda query, context="": _llm_chat(config_manager, query, context)
    ))

    # ========== å·¥å…·7: åŸå¸‚æ¨è ==========
    # æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆä¸ªæ€§åŒ–åŸå¸‚æ¨è
    tools.append((
        ToolInfo(
            name="generate_city_recommendation",
            description="æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆä¸ªæ€§åŒ–åŸå¸‚æ¨è",
            parameters={
                'type': 'object',
                'properties': {
                    'user_query': {'type': 'string', 'description': 'ç”¨æˆ·åŸå§‹éœ€æ±‚'},
                    'available_cities': {
                        'type': 'array',
                        'items': {'type': 'string'},
                        'description': 'å¯é€‰åŸå¸‚åˆ—è¡¨'
                    }
                },
                'required': ['user_query', 'available_cities']
            },
            required_params=['user_query', 'available_cities'],
            category='ai',
            tags=['recommend', 'city', 'llm']
        ),
        lambda user_query, available_cities: _generate_recommendation(config_manager, user_query, available_cities)
    ))

    # ========== å·¥å…·8: è·¯çº¿è§„åˆ’ ==========
    # æ ¹æ®åŸå¸‚æ™¯ç‚¹ä¿¡æ¯ç”Ÿæˆè¯¦ç»†è·¯çº¿è§„åˆ’
    tools.append((
        ToolInfo(
            name="generate_route_plan",
            description="æ ¹æ®åŸå¸‚æ™¯ç‚¹ä¿¡æ¯ç”Ÿæˆè¯¦ç»†è·¯çº¿è§„åˆ’",
            parameters={
                'type': 'object',
                'properties': {
                    'city': {'type': 'string', 'description': 'ç›®æ ‡åŸå¸‚'},
                    'days': {'type': 'integer', 'description': 'æ—…è¡Œå¤©æ•°'},
                    'preferences': {'type': 'string', 'description': 'ç”¨æˆ·åå¥½'}
                },
                'required': ['city', 'days']
            },
            required_params=['city', 'days'],
            category='ai',
            tags=['route', 'plan', 'llm']
        ),
        lambda city, days, preferences="": _generate_route_plan(config_manager, city, days, preferences)
    ))

    return tools


# ==============================================================================
# å·¥å…·æ‰§è¡Œå‡½æ•°
# è¿™äº›å‡½æ•°æ˜¯å·¥å…·çš„å…·ä½“å®ç°ï¼Œç”± create_travel_tools ä¸­å®šä¹‰çš„ lambda è°ƒç”¨
# ==============================================================================

def _search_cities(config_manager, interests: List[str] = None,
                   budget: tuple = None, season: str = None) -> Dict[str, Any]:
    """
    æœç´¢åŒ¹é…çš„åŸå¸‚

    æ ¹æ®ç”¨æˆ·çš„å…´è¶£æ ‡ç­¾ã€é¢„ç®—èŒƒå›´å’Œå‡ºè¡Œå­£èŠ‚ï¼Œä»æ•°æ®åº“ä¸­æœç´¢åŒ¹é…çš„åŸå¸‚ã€‚

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        interests: ç”¨æˆ·å…´è¶£æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ ["ç¾é£Ÿ", "å†å²æ–‡åŒ–"]
        budget: é¢„ç®—èŒƒå›´å…ƒç»„ (æœ€ä½, æœ€é«˜)ï¼Œå¦‚ (1000, 5000)
        season: å‡ºè¡Œå­£èŠ‚ï¼Œå¦‚ "æ˜¥å­£", "å¤å­£"

    Returns:
        Dict: åŒ…å«æœç´¢ç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¸º {'success': bool, 'cities': [...]}

    Examples:
        >>> result = _search_cities(None, ["ç¾é£Ÿ"], (1000, 3000), "æ˜¥å­£")
        >>> if result['success']:
        ...     for city in result['cities']:
        ...         print(city['name'])
    """
    from environment.travel_data import TravelData
    env = TravelData(config_manager)
    return env.search_cities(interests, budget, season)


def _query_attractions(config_manager, cities: List[str]) -> Dict[str, Any]:
    """
    æŸ¥è¯¢åŸå¸‚æ™¯ç‚¹ä¿¡æ¯

    è·å–æŒ‡å®šåŸå¸‚çš„æ™¯ç‚¹åˆ—è¡¨å’Œç›¸å…³è¯¦ç»†ä¿¡æ¯ã€‚

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        cities: è¦æŸ¥è¯¢çš„åŸå¸‚åç§°åˆ—è¡¨

    Returns:
        Dict: åŒ…å«æ™¯ç‚¹ä¿¡æ¯çš„å­—å…¸ï¼Œæ ¼å¼ä¸º {'success': bool, 'data': {...}}

    Examples:
        >>> result = _query_attractions(None, ["åŒ—äº¬", "ä¸Šæµ·"])
        >>> if result['success']:
        ...     for city, info in result['data'].items():
        ...         print(f"{city}: {len(info.get('attractions', []))} ä¸ªæ™¯ç‚¹")
    """
    from environment.travel_data import TravelData
    env = TravelData(config_manager)
    return env.query_attractions(cities)


def _generate_route(config_manager, city: str, days: int) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ—…æ¸¸è·¯çº¿è§„åˆ’

    æ ¹æ®åŸå¸‚ä¿¡æ¯å’Œæ—…è¡Œå¤©æ•°ï¼Œè‡ªåŠ¨ç”Ÿæˆæ¯æ—¥çš„æ™¯ç‚¹æ¸¸è§ˆè·¯çº¿ã€‚

    ç®—æ³•é€»è¾‘ï¼š
    1. è·å–åŸå¸‚åŸºæœ¬ä¿¡æ¯
    2. æå–åŸå¸‚æ™¯ç‚¹åˆ—è¡¨
    3. æŒ‰å¤©æ•°åˆ†é…æ™¯ç‚¹ï¼Œç”Ÿæˆæ¯æ—¥è·¯çº¿
    4. è®¡ç®—é¢„ä¼°è´¹ç”¨

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        city: ç›®æ ‡åŸå¸‚åç§°
        days: æ—…è¡Œå¤©æ•°

    Returns:
        Dict: è·¯çº¿è§„åˆ’ç»“æœï¼ŒåŒ…å«ï¼š
        - success: æ˜¯å¦æˆåŠŸ
        - city: åŸå¸‚åç§°
        - route_plan: æ¯æ—¥è·¯çº¿åˆ—è¡¨
        - total_cost_estimate: è´¹ç”¨ä¼°ç®—

    Examples:
        >>> result = _generate_route(None, "åŒ—äº¬", 3)
        >>> if result['success']:
        ...     for day in result['route_plan']:
        ...         print(f"ç¬¬{day['day']}å¤©: {day['schedule']}")
    """
    from environment.travel_data import TravelData
    env = TravelData(config_manager)
    result = env.get_city_info(city)
    if not result.get('success'):
        return result

    city_info = result.get('info', {})
    attractions = city_info.get('attractions', [])

    # ç”Ÿæˆè·¯çº¿è®¡åˆ’
    # ç­–ç•¥ï¼šæ¯å¤©åˆ†é…ä¸€ä¸ªä¸»è¦æ™¯ç‚¹ï¼ŒæŒ‰é¡ºåºå¾ªç¯
    route_plan = []
    for i in range(min(days, len(attractions))):
        attr = attractions[i] if i < len(attractions) else {'name': 'è‡ªç”±æ´»åŠ¨'}
        route_plan.append({
            'day': i + 1,
            'attractions': [attr['name']] if isinstance(attr, dict) else [attr],
            'schedule': f'æ¸¸è§ˆ{attr.get("name", "è‡ªç”±æ´»åŠ¨")}'
        })

    # è®¡ç®—è´¹ç”¨ä¼°ç®—
    # é—¨ç¥¨è´¹ç”¨ + æ¯æ—¥å¹³å‡èŠ±è´¹
    return {
        'success': True,
        'city': city,
        'route_plan': route_plan,
        'total_cost_estimate': {
            'tickets': sum(a.get('ticket', 0) for a in attractions[:days]),
            'total': sum(a.get('ticket', 0) for a in attractions[:days]) +
                     city_info.get('avg_budget_per_day', 400) * days
        }
    }


def _calculate_budget(config_manager, city: str, days: int) -> Dict[str, Any]:
    """
    è®¡ç®—æ—…æ¸¸é¢„ç®—

    æ ¹æ®åŸå¸‚ç‰©ä»·æ°´å¹³å’Œæ—…è¡Œå¤©æ•°ï¼Œè®¡ç®—é¢„è®¡èŠ±è´¹ã€‚

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        city: ç›®æ ‡åŸå¸‚
        days: æ—…è¡Œå¤©æ•°

    Returns:
        Dict: é¢„ç®—è®¡ç®—ç»“æœï¼ŒåŒ…å«å„é¡¹ç›®çš„è´¹ç”¨æ˜ç»†
    """
    from environment.travel_data import TravelData
    env = TravelData(config_manager)
    return env.calculate_budget(city, days)


def _get_city_info(config_manager, city: str) -> Dict[str, Any]:
    """
    è·å–åŸå¸‚è¯¦ç»†ä¿¡æ¯

    è·å–æŒ‡å®šåŸå¸‚çš„å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŒºåŸŸã€æ ‡ç­¾ã€å­£èŠ‚ã€é¢„ç®—ã€æ™¯ç‚¹ç­‰ã€‚

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        city: åŸå¸‚åç§°

    Returns:
        Dict: åŸå¸‚è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
        - success: æ˜¯å¦æˆåŠŸ
        - city: åŸå¸‚åç§°
        - info: è¯¦ç»†ä¿¡æ¯å­—å…¸
    """
    from environment.travel_data import TravelData
    env = TravelData(config_manager)
    return env.get_city_info(city)


def _llm_chat(config_manager, query: str, context: str = "") -> Dict[str, Any]:
    """
    LLM å¯¹è¯å›ç­”

    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œå¤„ç†ç”¨æˆ·çš„ä¸€èˆ¬æ€§é—®é¢˜ã€‚

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        query: ç”¨æˆ·é—®é¢˜
        context: å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰

    Returns:
        Dict: LLM å›ç­”ç»“æœï¼Œæ ¼å¼ä¸º {'success': bool, 'response': str}
    """
    llm_config = config_manager.get_default_model_config()
    llm_client = LLMClient(llm_config)

    messages = [{"role": "user", "content": query}]
    # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯ä¸­
    if context:
        messages.insert(0, {"role": "system", "content": context})

    result = llm_client.chat(messages)

    # æ ‡å‡†åŒ–è¿”å›æ ¼å¼
    if isinstance(result, dict):
        if result.get('success') and 'content' in result:
            return {'success': True, 'response': result['content']}
        elif 'error' in result:
            return {'success': False, 'response': result['error']}
    return result


def _generate_recommendation(config_manager, user_query: str,
                             available_cities: List[str]) -> Dict[str, Any]:
    """
    ç”ŸæˆåŸå¸‚æ¨è

    æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œå¯ç”¨åŸå¸‚åˆ—è¡¨ï¼Œä½¿ç”¨ LLM ç”Ÿæˆä¸ªæ€§åŒ–æ¨èã€‚

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        user_query: ç”¨æˆ·åŸå§‹éœ€æ±‚æè¿°
        available_cities: å¯é€‰åŸå¸‚åˆ—è¡¨

    Returns:
        Dict: æ¨èç»“æœï¼ŒåŒ…å«æ¨èçš„åŸå¸‚åˆ—è¡¨å’Œç†ç”±
    """
    llm_config = config_manager.get_default_model_config()
    llm_client = LLMClient(llm_config)
    return llm_client.generate_travel_recommendation(user_query, "", available_cities)


def _generate_route_plan(config_manager, city: str, days: int,
                         preferences: str = "") -> Dict[str, Any]:
    """
    ç”Ÿæˆè¯¦ç»†è·¯çº¿è®¡åˆ’

    ä½¿ç”¨ LLM æ ¹æ®åŸå¸‚æ™¯ç‚¹ä¿¡æ¯ç”Ÿæˆè¯¦ç»†çš„æ¯æ—¥è¡Œç¨‹è§„åˆ’ã€‚

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        city: ç›®æ ‡åŸå¸‚
        days: æ—…è¡Œå¤©æ•°
        preferences: ç”¨æˆ·åå¥½æè¿°

    Returns:
        Dict: è¯¦ç»†è·¯çº¿è®¡åˆ’
    """
    city_info = config_manager.get_city_info(city)
    if not city_info:
        return {'success': False, 'error': f'æœªæ‰¾åˆ°åŸå¸‚: {city}'}

    attractions = city_info.get('attractions', [])
    llm_config = config_manager.get_default_model_config()
    llm_client = LLMClient(llm_config)
    return llm_client.generate_route_plan(city, days, attractions, preferences)


# ==============================================================================
# ReAct æ—…æ¸¸åŠ©æ‰‹ä¸»ç±»
# ==============================================================================

class ReActTravelAgent:
    """
    ReAct æ—…æ¸¸åŠ©æ‰‹ Agent

    è¯¥ç±»æ˜¯æ—…æ¸¸åŠ©æ‰‹çš„æ ¸å¿ƒå…¥å£ï¼Œåè°ƒä»¥ä¸‹ç»„ä»¶å·¥ä½œï¼š
    1. ReActAgent: è´Ÿè´£æ¨ç†å’Œå·¥å…·è°ƒç”¨çš„å¾ªç¯
    2. MemoryManager: è´Ÿè´£å¯¹è¯å†å²çš„å­˜å‚¨å’Œç®¡ç†
    3. LLMClient: è´Ÿè´£ä¸å¤§è¯­è¨€æ¨¡å‹é€šä¿¡
    4. ConfigManager: è´Ÿè´£é…ç½®ä¿¡æ¯çš„è¯»å–

    å¤„ç†æµç¨‹ï¼š
    1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
    2. è°ƒç”¨ ReActAgent æ‰§è¡Œæ¨ç†å¾ªç¯
    3. æ”¶é›†å·¥å…·æ‰§è¡Œç»“æœ
    4. ä½¿ç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”
    5. è¿”å›ç»“æ„åŒ–ç»“æœ

    Attributes:
        config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
        memory_manager: å¯¹è¯å†å²ç®¡ç†å™¨
        llm_client: LLM å®¢æˆ·ç«¯å®ä¾‹
        react_agent: ReAct æ™ºèƒ½ä½“å®ä¾‹

    Examples:
        >>> agent = ReActTravelAgent(config_path="config/llm_config.yaml")
        >>> result = await agent.process("åŒ—äº¬ä¸‰æ—¥æ¸¸æ¨è")
        >>> print(result["answer"])
    """

    def __init__(self, config_path: str = "config/llm_config.yaml",
                 model_id: Optional[str] = None,
                 max_steps: int = 10):
        """
        åˆå§‹åŒ–æ—…æ¸¸åŠ©æ‰‹

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            model_id: ä½¿ç”¨çš„æ¨¡å‹ IDï¼Œä¸º None åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            max_steps: ReAct å¾ªç¯çš„æœ€å¤§æ‰§è¡Œæ­¥éª¤æ•°
        """
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager(config_path)

        # åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
        # max_working_memory æ§åˆ¶çŸ­æœŸå·¥ä½œè®°å¿†çš„å¤§å°
        memory_config = self.config_manager.agent_config.get('max_working_memory', 10)
        self.memory_manager = MemoryManager(
            max_working_memory=memory_config
        )

        # è·å–æ¨¡å‹é…ç½®å¹¶åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        if model_id:
            llm_config = self.config_manager.get_model_config(model_id)
        else:
            llm_config = self.config_manager.get_default_model_config()

        self.llm_client = LLMClient(llm_config)

        # ä¼ é€’ llm_client ç»™ ReActAgentï¼Œä½¿å…¶èƒ½ä½¿ç”¨ LLM è¿›è¡Œæ€è€ƒ
        # è¿™æ˜¯ ReAct æ¨¡å¼çš„å…³é”®ï¼šè®©æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»æ€è€ƒå’Œè§„åˆ’
        self.react_agent = ReActAgent(
            name="TravelReActAgent",
            max_steps=max_steps,
            max_reasoning_depth=5,
            llm_client=self.llm_client
        )

        # æ³¨å†Œå·¥å…·å’Œå›è°ƒ
        self._register_tools()
        self._register_callbacks()

    def _register_tools(self) -> None:
        """
        æ³¨å†Œæ—…æ¸¸å·¥å…·åˆ° ReActAgent

        å°† create_travel_tools åˆ›å»ºçš„æ‰€æœ‰å·¥å…·æ³¨å†Œåˆ° ReActAgent çš„å·¥å…·æ³¨å†Œè¡¨ä¸­ã€‚
        """
        tools = create_travel_tools(self.config_manager)
        for tool_info, executor in tools:
            self.react_agent.register_tool(tool_info, executor)

    def _register_callbacks(self) -> None:
        """
        æ³¨å†Œäº‹ä»¶å›è°ƒå‡½æ•°

        ç”¨äºå°† ReActAgent çš„æ€è€ƒå’Œè¡ŒåŠ¨äº‹ä»¶åŒæ­¥åˆ°è®°å¿†ç®¡ç†å™¨ä¸­ï¼Œ
        ä»¥ä¾¿ç»´æŠ¤å®Œæ•´çš„å¯¹è¯å†å²ã€‚
        """
        def on_thought(thought: Thought):
            """æ€è€ƒäº‹ä»¶å›è°ƒï¼šå°†æ€è€ƒå†…å®¹æ·»åŠ åˆ°è®°å¿†"""
            self.memory_manager.add_message('assistant', f"[æ€è€ƒ] {thought.content}")

        def on_action(action: Action):
            """è¡ŒåŠ¨äº‹ä»¶å›è°ƒï¼šæ ¹æ®çŠ¶æ€è®°å½•ä¸åŒæ¶ˆæ¯"""
            if action.status == ActionStatus.RUNNING:
                self.memory_manager.add_message('assistant', f"[è¡ŒåŠ¨] æ‰§è¡Œå·¥å…·: {action.tool_name}")
            elif action.status == ActionStatus.SUCCESS:
                self.memory_manager.add_message('assistant', f"[å®Œæˆ] {action.tool_name}")
            elif action.status == ActionStatus.FAILED:
                self.memory_manager.add_message('assistant', f"[å¤±è´¥] {action.tool_name}: {action.error}")

        self.react_agent.add_thought_callback(on_thought)
        self.react_agent.add_action_callback(on_action)

    async def process(self, user_input: str) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆéæµå¼ç‰ˆæœ¬ï¼‰

        è¿™æ˜¯ä¸»è¦çš„å¤„ç†å…¥å£ï¼Œæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œæ‰§è¡Œå®Œæ•´çš„ ReAct å¾ªç¯ï¼Œ
        å¹¶è¿”å›ç»“æ„åŒ–çš„å¤„ç†ç»“æœã€‚

        Args:
            user_input: ç”¨æˆ·çš„è¾“å…¥æ–‡æœ¬

        Returns:
            Dict: å¤„ç†ç»“æœï¼ŒåŒ…å«ï¼š
            - success: æ˜¯å¦æˆåŠŸ
            - answer: ç”Ÿæˆçš„å›ç­”
            - reasoning: æ¨ç†è¿‡ç¨‹ä¿¡æ¯
            - history: æ‰§è¡Œå†å²

        Examples:
            >>> result = await agent.process("äº‘å—æ—…æ¸¸æ¨è")
            >>> if result["success"]:
            ...     print(result["answer"])
        """
        import logging
        logger = logging.getLogger(__name__)

        logger.info(f"[Agent] å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")

        try:
            # 1. å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²
            self.memory_manager.add_message('user', user_input)

            # 2. æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context = {
                'user_query': user_input,
                'user_preference': self.memory_manager.get_user_preference()
            }

            # 3. æ‰§è¡Œ ReAct æ¨ç†å¾ªç¯
            result = await self.react_agent.run(user_input, context)
            logger.info(f"[Agent] ReAct æ‰§è¡Œå®Œæˆ, success={result.get('success')}, steps={len(result.get('history', []))}")

            if result.get('success'):
                # 4. æå–ç»“æœ
                history = result.get('history', [])
                reasoning_text = self._build_reasoning_text(history)
                answer = self._extract_answer(history)
                logger.info(f"[Agent] æå–åˆ°ç­”æ¡ˆ: {answer[:100]}...")

                # 5. æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
                self.memory_manager.add_message('assistant', answer)

                return {
                    "success": True,
                    "answer": answer,
                    "reasoning": {
                        "text": reasoning_text,
                        "total_steps": len(history),
                        "tools_used": self._extract_tools_used(history)
                    },
                    "history": history
                }
            else:
                return {
                    "success": False,
                    "error": result.get('error', 'å¤„ç†å¤±è´¥'),
                    "reasoning": None,
                    "history": result.get('history', [])
                }

        except Exception as e:
            logger.error(f"[Agent] å¤„ç†å¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": f"å¤„ç†å¤±è´¥: {str(e)}",
                "reasoning": None
            }

    def process_sync(self, user_input: str) -> Dict[str, Any]:
        """
        åŒæ­¥å¤„ç†ç”¨æˆ·è¾“å…¥

        ç”¨äº gRPC è°ƒç”¨ç­‰éœ€è¦åŒæ­¥æ¥å£çš„åœºæ™¯ã€‚
        å†…éƒ¨é€šè¿‡ asyncio.run() åŒ…è£…å¼‚æ­¥çš„ process æ–¹æ³•ã€‚

        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬

        Returns:
            Dict: å¤„ç†ç»“æœï¼ŒåŒ process æ–¹æ³•çš„è¿”å›æ ¼å¼
        """
        import asyncio
        return asyncio.run(self.process(user_input))

    async def process_stream(self, user_input: str, answer_callback=None, done_callback=None, thinking_callback=None):
        """
        æµå¼å¤„ç†ç”¨æˆ·è¾“å…¥

        ä½¿ç”¨çœŸæ­£çš„ token çº§åˆ«æµå¼è¾“å‡ºï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚
        ç‰¹ç‚¹ï¼š
        - å®æ—¶è¾“å‡ºï¼šæ¯ä¸ª token ç”Ÿæˆåç«‹å³é€šè¿‡å›è°ƒå‘é€
        - çœŸæ­£çš„æµå¼ï¼šä½¿ç”¨ LLM å®¢æˆ·ç«¯çš„ chat_stream æ–¹æ³•
        - å›è°ƒæœºåˆ¶ï¼šé€šè¿‡å›è°ƒå‡½æ•°å®ç°æ•°æ®æ¨é€

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            answer_callback: å›ç­”å†…å®¹å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶å•ä¸ª token (str)
            done_callback: å®Œæˆå›è°ƒå‡½æ•°ï¼Œæ¥æ”¶æœ€ç»ˆç»“æœ (Dict)
            thinking_callback: æ€è€ƒå†…å®¹å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶æ€è€ƒå†…å®¹ (str) å’Œè€—æ—¶ (float)

        Returns:
            Dict: æœ€ç»ˆå¤„ç†ç»“æœ

        Examples:
            >>> async def on_token(token):
            ...     print(token, end="", flush=True)
            >>> async def on_done(result):
            ...     print("\\nå®Œæˆ!")
            >>> await agent.process_stream("åŒ—äº¬æ—…æ¸¸", answer_callback=on_token, done_callback=on_done)
        """
        import logging
        import time as time_module
        logger = logging.getLogger(__name__)

        logger.info(f"[Agent] å¼€å§‹æµå¼å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
        start_time = time_module.time()

        try:
            # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²
            self.memory_manager.add_message('user', user_input)

            context = {
                'user_query': user_input,
                'user_preference': self.memory_manager.get_user_preference()
            }

            # å…ˆè¿è¡Œ ReAct agent è·å–æ€è€ƒå†å²
            # è®¾ç½®æ€è€ƒæµå¼å›è°ƒ
            if hasattr(self.react_agent, 'set_think_stream_callback') and thinking_callback:
                self.react_agent.set_think_stream_callback(thinking_callback)

            result = await self.react_agent.run(user_input, context)
            logger.info(f"[Agent] ReAct æ‰§è¡Œå®Œæˆ, success={result.get('success')}, steps={len(result.get('history', []))}")

            if result.get('success'):
                history = result.get('history', [])
                reasoning_text = self._build_reasoning_text(history)
                answer = self._extract_answer(history)

                self.memory_manager.add_message('assistant', answer)

                # æ„å»º LLM æ¶ˆæ¯
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œæä¾›è¯¦ç»†ã€å‡†ç¡®çš„æ—…æ¸¸å»ºè®®å’Œè§„åˆ’ã€‚å›ç­”è¦ç®€æ´æ˜äº†ï¼Œæ¡ç†æ¸…æ™°ã€‚"""
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input}
                ]

                logger.info(f"[Agent] å¼€å§‹æµå¼ç”Ÿæˆç­”æ¡ˆ...")

                # ä½¿ç”¨ LLM å®¢æˆ·ç«¯çš„æµå¼æ–¹æ³•
                if hasattr(self.llm_client, 'chat_stream'):
                    token_count = 0
                    accumulated_answer = ""

                    # éå†æµå¼å“åº”
                    for token in self.llm_client.chat_stream(messages, temperature=0.7):
                        token_count += 1
                        accumulated_answer += token

                        # ç«‹å³å‘é€æ¯ä¸ª token
                        if answer_callback:
                            answer_callback(token)

                        # çŸ­æš‚å»¶è¿Ÿï¼Œç¡®ä¿å‰ç«¯æœ‰è¶³å¤Ÿæ—¶é—´å¤„ç†
                        await asyncio.sleep(0.01)

                    answer = accumulated_answer
                    logger.info(f"[Agent] æµå¼ç”Ÿæˆå®Œæˆ, å…± {token_count} tokens")

                else:
                    # å›é€€åˆ°éæµå¼
                    logger.warning("[Agent] LLM å®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼ï¼Œä½¿ç”¨æ‰¹é‡å‘é€")
                    chunks = self._split_into_chunks(answer)
                    for chunk in chunks:
                        if answer_callback:
                            answer_callback(chunk)
                        await asyncio.sleep(0.02)

                elapsed = time_module.time() - start_time
                logger.info(f"[Agent] æ€»è€—æ—¶: {elapsed:.2f}ç§’")

                final_result = {
                    "success": True,
                    "answer": answer,
                    "reasoning": {
                        "text": reasoning_text,
                        "total_steps": len(history),
                        "tools_used": self._extract_tools_used(history)
                    },
                    "history": history
                }

                if done_callback:
                    done_callback(final_result)

                return final_result
            else:
                final_result = {
                    "success": False,
                    "error": result.get('error', 'å¤„ç†å¤±è´¥'),
                    "reasoning": None,
                    "history": result.get('history', [])
                }
                if done_callback:
                    done_callback(final_result)
                return final_result

        except Exception as e:
            logger.error(f"[Agent] å¤„ç†å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            error_result = {
                "success": False,
                "error": f"å¤„ç†å¤±è´¥: {str(e)}",
                "reasoning": None
            }
            if done_callback:
                done_callback(error_result)
            return error_result

    def _split_into_chunks(self, text: str, chunk_size: int = 3) -> List[str]:
        """
        å°†æ–‡æœ¬æ‹†åˆ†æˆå°å—ç”¨äºæµå¼è¾“å‡º

        å½“ LLM ä¸æ”¯æŒæµå¼è¾“å‡ºæ—¶ï¼Œä½¿ç”¨æ­¤æ–¹æ³•è¿›è¡Œæ¨¡æ‹Ÿæµå¼ã€‚
        æ‹†åˆ†ç­–ç•¥ï¼š
        1. ä¼˜å…ˆåœ¨æ ‡ç‚¹ç¬¦å·å¤„æ–­å¼€
        2. æ§åˆ¶æ¯å—æœ€å¤§é•¿åº¦
        3. ç¡®ä¿ä¸­è‹±æ–‡éƒ½èƒ½æ­£ç¡®å¤„ç†

        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆä¸­æ–‡å­—ç¬¦ï¼‰ï¼Œé»˜è®¤3ä¸ª

        Returns:
            æ–‡æœ¬å—åˆ—è¡¨

        Examples:
            >>> chunks = agent._split_into_chunks("ä½ å¥½ä¸–ç•Œï¼å†è§ã€‚")
            >>> print(chunks)  # ['ä½ å¥½', 'ä¸–ç•Œ', 'ï¼', 'å†è§', 'ã€‚']
        """
        if not text:
            return []

        chunks = []
        i = 0

        while i < len(text):
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ–­ç‚¹ï¼ˆæ ‡ç‚¹æˆ–æ¢è¡Œï¼‰
            chunk_end = min(i + 20, len(text))  # æœ€å¤§20ä¸ªå­—ç¬¦

            # ä»åå¾€å‰æ‰¾åˆé€‚çš„æ–­ç‚¹
            for j in range(chunk_end, i, -1):
                char = text[j - 1]
                # ä¸­æ–‡æ ‡ç‚¹ä½œä¸ºæ–­ç‚¹
                if char in 'ã€‚ï¼ï¼Ÿï¼›ï¼šã€\n':
                    chunk_end = j
                    break
                # è‹±æ–‡æ ‡ç‚¹ä¹Ÿä½œä¸ºæ–­ç‚¹
                if char in '.!?:;,' and j > i + 3:
                    chunk_end = j
                    break

            # ç¡®ä¿è‡³å°‘è¿”å›ä¸€ä¸ªå­—ç¬¦
            if chunk_end <= i:
                chunk_end = min(i + 1, len(text))

            chunk = text[i:chunk_end]
            chunks.append(chunk)
            i = chunk_end

        # å¦‚æœåˆ†å—å¤ªå¤§ï¼Œè¿›ä¸€æ­¥æ‹†åˆ†
        final_chunks = []
        for chunk in chunks:
            while len(chunk) > 15:  # å¦‚æœå—å¤ªå¤§ï¼ŒæŒ‰æ›´å°å•ä½æ‹†åˆ†
                final_chunks.append(chunk[:8])  # 8ä¸ªå­—ç¬¦
                chunk = chunk[8:]
            if chunk:
                final_chunks.append(chunk)

        return final_chunks if final_chunks else [text]

    def _build_reasoning_text(self, history: List[Dict]) -> str:
        """
        æ„å»ºæ¨ç†è¿‡ç¨‹æ–‡æœ¬

        å°† ReAct æ‰§è¡Œå†å²æ ¼å¼åŒ–ä¸ºå¯è¯»çš„æ¨ç†è¿‡ç¨‹æè¿°ã€‚

        Args:
            history: ReAct æ‰§è¡Œå†å²åˆ—è¡¨

        Returns:
            str: æ ¼å¼åŒ–åçš„æ¨ç†è¿‡ç¨‹æ–‡æœ¬ï¼ˆMarkdown æ ¼å¼ï¼‰
        """
        if not history:
            return "<thinking>\n[Timestamp: {timestamp}]\n\n[Intent Analysis]\nNo reasoning history available.\n\n[Context Evaluation]\nNo context available.\n\n[Response Planning]\nUnable to generate response.\n\n[Constraint Check]\nNo constraints checked.\n</thinking>".format(
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            )

        intent_analysis = []
        context_evaluation = []
        response_planning = []
        constraint_check = []

        # éå†å†å²ï¼ŒæŒ‰ç±»å‹åˆ†ç±»
        for i, step in enumerate(history):
            thought = step.get('thought', {})
            action = step.get('action', {})

            thought_type = thought.get('type', 'UNKNOWN')
            thought_content = thought.get('content', '')
            action_name = action.get('tool_name', '')
            action_status = action.get('status', 'PENDING')
            result = action.get('result', {})

            if thought_type == 'ANALYSIS':
                if thought_content:
                    intent_analysis.append(f"Step {i + 1}: {thought_content}")
            elif thought_type == 'PLANNING':
                if thought_content:
                    response_planning.append(f"Step {i + 1}: {thought_content}")
            elif thought_type == 'INFERENCE':
                if thought_content:
                    context_evaluation.append(f"Step {i + 1}: {thought_content}")
                if action_name and action_name != 'none':
                    status_str = 'SUCCESS' if action_status == 'SUCCESS' else 'FAILED' if action_status == 'FAILED' else 'RUNNING'
                    context_evaluation.append(f"  - Tool: {action_name} [{status_str}]")
            elif thought_type == 'REFLECTION':
                if thought_content:
                    constraint_check.append(f"Step {i + 1}: {thought_content}")

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æ„å»ºå„éƒ¨åˆ†å†…å®¹
        intent_section = "[Intent Analysis]\n"
        if intent_analysis:
            intent_section += "\n".join(intent_analysis)
        else:
            intent_section += f"User query analysis based on {len(history)} reasoning steps.\n"

        context_section = "[Context Evaluation]\n"
        if context_evaluation:
            context_section += "\n".join(context_evaluation)
        else:
            context_section += "No explicit context evaluation steps recorded."

        response_section = "[Response Planning]\n"
        if response_planning:
            response_section += "\n".join(response_planning)
        else:
            response_section += "Response generation based on tool execution results."

        constraint_section = "[Constraint Check]\n"
        if constraint_check:
            constraint_section += "\n".join(constraint_check)
        else:
            constraint_section += "All constraints satisfied.\n"
            constraint_section += f"- Total reasoning steps: {len(history)}\n"
            constraint_section += f"- Tools executed: {len(self._extract_tools_used(history))}\n"
            constraint_section += "- Response format: Standard text response"

        thinking_content = f"""[Timestamp: {timestamp}]

{intent_section}

{context_section}

{response_section}

{constraint_section}"""

        return f"<thinking>\n{thinking_content}\n</thinking>"

    def _extract_tools_used(self, history: List[Dict]) -> List[str]:
        """
        æå–ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨

        ä»æ‰§è¡Œå†å²ä¸­æ”¶é›†æ‰€æœ‰è¢«è°ƒç”¨çš„å·¥å…·åç§°ã€‚

        Args:
            history: æ‰§è¡Œå†å²åˆ—è¡¨

        Returns:
            List[str]: ä½¿ç”¨çš„å·¥å…·åç§°åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        """
        tools = []
        for step in history:
            action = step.get('action', {})
            tool_name = action.get('tool_name', '')
            if tool_name and tool_name not in tools and tool_name != 'none':
                tools.append(tool_name)
        return tools

    def _extract_answer(self, history: List[Dict]) -> str:
        """
        æå–æœ€ç»ˆå›ç­”

        ä»æ‰§è¡Œå†å²ä¸­æå–æœ€ç»ˆçš„å›ç­”å†…å®¹ã€‚
        ç­–ç•¥ï¼š
        1. æ”¶é›†æ‰€æœ‰æˆåŠŸçš„å·¥å…·æ‰§è¡Œç»“æœ
        2. ä½¿ç”¨ LLM ç”Ÿæˆæ´»æ³¼ã€ç»“æ„åŒ–çš„å›ç­”

        Args:
            history: æ‰§è¡Œå†å²åˆ—è¡¨

        Returns:
            str: æœ€ç»ˆå›ç­”æ–‡æœ¬
        """
        # æ”¶é›†æ‰€æœ‰å·¥å…·æ‰§è¡Œç»“æœ
        tool_results = []
        has_successful_tools = False

        for step in reversed(history):
            action = step.get('action', {})
            if action.get('status') == 'SUCCESS':
                has_successful_tools = True
                result = action.get('result', {})
                tool_name = action.get('tool_name', '')
                if result:
                    tool_results.append({
                        'tool': tool_name,
                        'result': result
                    })

        # å¦‚æœæœ‰å·¥å…·æ‰§è¡Œç»“æœï¼Œä½¿ç”¨ LLM ç”Ÿæˆæ´»æ³¼çš„å›ç­”
        if has_successful_tools:
            return self._generate_answer(history)

        # å¦åˆ™è¿”å›é»˜è®¤æ¶ˆæ¯
        return 'è®©æˆ‘æ¥å¸®ä½ è§„åˆ’è¿™æ¬¡æ—…è¡Œå§ï¼ğŸ‰'

    def _format_attractions_response(self, tool_result: Dict) -> str:
        """
        æ ¼å¼åŒ–æ™¯ç‚¹å“åº”æ•°æ®

        å°†æ™¯ç‚¹æŸ¥è¯¢ç»“æœæ ¼å¼åŒ–ä¸ºå¯è¯»çš„æ–‡æœ¬ã€‚

        Args:
            tool_result: å·¥å…·è¿”å›çš„åŸå§‹ç»“æœ

        Returns:
            str: æ ¼å¼åŒ–åçš„æ™¯ç‚¹æè¿°æ–‡æœ¬
        """
        lines = []

        # å…¼å®¹æ–°æ—§ä¸¤ç§æ•°æ®æ ¼å¼
        if 'cities' in tool_result:
            data = tool_result['cities']
        elif 'data' in tool_result:
            data = tool_result['data']
        else:
            data = tool_result

        if not data:
            return "æœªæ‰¾åˆ°ç›¸å…³æ™¯ç‚¹ä¿¡æ¯"

        for city, data_item in data.items():
            region = data_item.get('region', '') if isinstance(data_item, dict) else ''
            region_str = f" (æ¥è‡ª{region}åœ°åŒº)" if region else ""
            lines.append(f"\n## {city}{region_str}")
            attractions = data_item.get('attractions', []) if isinstance(data_item, dict) else []
            if attractions:
                lines.append("\n### æ™¯ç‚¹æ¨èï¼š")
                for i, attr in enumerate(attractions[:10], 1):
                    name = attr.get('name', 'æœªçŸ¥æ™¯ç‚¹')
                    desc = attr.get('description', '')[:100]
                    ticket = attr.get('ticket', 0)
                    lines.append(f"{i}. **{name}**")
                    if desc:
                        lines.append(f"   - {desc}")
                    if ticket > 0:
                        lines.append(f"   - é—¨ç¥¨: Â¥{ticket}")
            else:
                lines.append("  æš‚æ— æ™¯ç‚¹ä¿¡æ¯")

        return '\n'.join(lines) if lines else "æœªæ‰¾åˆ°ç›¸å…³æ™¯ç‚¹ä¿¡æ¯"

    def _generate_answer(self, history: List[Dict], intent: IntentResult = None) -> str:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”

        æ ¹æ®å·¥å…·æ‰§è¡Œç»“æœå’Œç”¨æˆ·æ„å›¾ï¼Œç”Ÿæˆç»“æ„åŒ–ã€é£æ ¼åŒ–çš„å›ç­”ã€‚

        Args:
            history: æ‰§è¡Œå†å²åˆ—è¡¨
            intent: æ„å›¾è¯†åˆ«ç»“æœï¼ˆå¯é€‰ï¼‰

        Returns:
            str: ç”Ÿæˆçš„å›ç­”æ–‡æœ¬
        """
        try:
            tool_results = []
            for step in history:
                action = step.get('action', {})
                if action.get('status') == 'SUCCESS' and action.get('result'):
                    tool_results.append({
                        'tool': action.get('tool_name', ''),
                        'result': action.get('result', {})
                    })

            # è·å–é£æ ¼é…ç½®
            if intent:
                # å®‰å…¨è·å– sentiment
                sentiment_value = intent.sentiment.value if hasattr(intent.sentiment, 'value') else str(intent.sentiment) if intent.sentiment else 'neutral'
                sentiment = SentimentType(sentiment_value) if sentiment_value in [e.value for e in SentimentType] else SentimentType.NEUTRAL
                style = style_manager.get_style_for_task(intent.intent.value, sentiment)
            else:
                style = style_manager.get_style_for_task("general_chat", SentimentType.NEUTRAL)

            # æ ¹æ®é£æ ¼è°ƒæ•´æ¸©åº¦
            temperature = style.temperature

            # æ„å»ºé£æ ¼åŒ–çš„ç³»ç»Ÿæç¤ºè¯
            system_prompt = self._build_style_prompt(style, intent)

            user_prompt = f"""æˆ‘æƒ³è¦è§„åˆ’ä¸€æ¬¡æ—…è¡Œï¼Œè¿™æ˜¯æˆ‘çš„æŸ¥è¯¢ç»“æœï¼š
{json.dumps(tool_results, ensure_ascii=False, indent=2)}

è¯·åªè¾“å‡ºJSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ã€‚"""

            result = self.llm_client.chat([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ], temperature=temperature)

            if result.get('success'):
                content = result.get('content', '')
                # å°è¯•è§£æJSON
                data = self._parse_json_response(content)
                if data:
                    return self._format_travel_response(data)
                return content
            return 'å¤„ç†å®Œæˆ'

        except Exception as e:
            return f'ç”Ÿæˆå›ç­”å¤±è´¥ï¼š{str(e)}'

    def _build_style_prompt(self, style: StyleConfig, intent: IntentResult = None) -> str:
        """
        æ ¹æ®é£æ ¼é…ç½®æ„å»ºç³»ç»Ÿæç¤ºè¯

        Args:
            style: é£æ ¼é…ç½®
            intent: æ„å›¾è¯†åˆ«ç»“æœ

        Returns:
            str: ç³»ç»Ÿæç¤ºè¯
        """
        # æ ¹æ®é£æ ¼é€‰æ‹©é—®å€™è¯­å’Œè§’è‰²è®¾å®š
        role_greetings = {
            "çƒ­æƒ…æ´»æ³¼": "ä½ æ˜¯ä¸€ä¸ªè¶…çº§çƒ­æƒ…ã€æ´»æ³¼çš„AIæ—…æ¸¸å°ä¼™ä¼´ï¼",
            "æ¸©æš–äº²åˆ‡": "ä½ æ˜¯ä¸€ä¸ªè´´å¿ƒã€æ¸©æš–çš„AIæ—…æ¸¸åŠ©æ‰‹ï¼",
            "ä¸“ä¸šæ­£å¼": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€å¯é çš„AIæ—…æ¸¸é¡¾é—®ã€‚",
            "ä¿çš®å¯çˆ±": "ä½ æ˜¯ä¸€ä¸ªå¯çˆ±åˆçƒ­æƒ…çš„æ—…è¡Œå°è¾¾äººï¼",
            "ç®€æ´æ˜äº†": "ä½ æ˜¯ä¸€ä¸ªç®€æ´é«˜æ•ˆçš„AIæ—…æ¸¸åŠ©æ‰‹ã€‚"
        }

        role = role_greetings.get(style.name, "ä½ æ˜¯ä¸€ä¸ªAIæ—…æ¸¸åŠ©æ‰‹")

        # æ ¹æ®é£æ ¼é€‰æ‹©è¯­æ°”å…³é”®è¯
        tone_keywords = {
            "çƒ­æƒ…æ´»æ³¼": "ä½¿ç”¨è½»æ¾æ´»æ³¼çš„è¯­æ°”ï¼Œå¤šç”¨å£è¯­åŒ–è¡¨è¾¾ã€‚é€‚å½“ä½¿ç”¨emojiè¡¨æƒ…ç¬¦å·å¢æ·»è¶£å‘³ã€‚ç”¨'å°ä¼™ä¼´'ã€'äº²'ã€'å“‡å¡'ç­‰äº²åˆ‡ç§°å‘¼ã€‚",
            "æ¸©æš–äº²åˆ‡": "ä½¿ç”¨æ¸©æŸ”äº²åˆ‡çš„è¯­æ°”ï¼Œåƒæœ‹å‹ä¸€æ ·èŠå¤©ã€‚é€‚å½“è¡¨è¾¾å…³å¿ƒå’Œç†è§£ã€‚è®©å¯¹è¯æ°›å›´è½»æ¾æ„‰å¿«ã€‚",
            "ä¸“ä¸šæ­£å¼": "ä½¿ç”¨ä¸“ä¸šã€æ¸…æ™°çš„è¯­è¨€ã€‚æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯ã€‚ä¿æŒç¤¼è²Œå’Œä¸“ä¸šçš„æ€åº¦ã€‚",
            "ä¿çš®å¯çˆ±": "ä½¿ç”¨ä¿çš®å¯çˆ±çš„è¯­æ°”ï¼Œå¯ä»¥é€‚å½“ç”¨ä¸€äº›æœ‰è¶£çš„ç½‘ç»œç”¨è¯­ã€‚å¤šå¤šä½¿ç”¨å¯çˆ±çš„emojiã€‚",
            "ç®€æ´æ˜äº†": "ä½¿ç”¨ç®€æ´ã€ç›´æ¥çš„è¯­è¨€ã€‚ä¸è¯´åºŸè¯ï¼Œç›´å¥”ä¸»é¢˜ã€‚é«˜æ•ˆä¼ é€’ä¿¡æ¯ã€‚"
        }

        tone = tone_keywords.get(style.name, "ä½¿ç”¨å‹å¥½çš„è¯­æ°”")

        # æ„å»ºæç¤ºè¯
        prompt = f"""{role}

ã€ä»»åŠ¡ã€‘
æ ¹æ®å·¥å…·æŸ¥è¯¢ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–çš„æ—…æ¸¸æ¨èä¿¡æ¯ã€‚

ã€è¯´è¯é£æ ¼ã€‘
- {tone}
- é€‚å½“åŠ å…¥æ—…è¡Œçš„æ°›å›´æ„Ÿæå†™
- é‡ç‚¹ä¿¡æ¯ç”¨**åŠ ç²—**æ ‡è®°

ã€è¾“å‡ºæ ¼å¼ã€‘
å¿…é¡»è¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•Markdownæ ¼å¼ï¼JSONç»“æ„å¦‚ä¸‹ï¼š
{{
    "opening": "å¼€åœºç™½ï¼Œä½¿ç”¨è½»æ¾æ´»æ³¼çš„è¯­æ°”",
    "cities": [
        {{
            "name": "åŸå¸‚å",
            "emoji": "åŸå¸‚emoji",
            "days": "æ¨èå¤©æ•°",
            "budget": "é¢„ç®—æè¿°",
            "season": "æœ€ä½³æ—…è¡Œå­£èŠ‚",
            "attractions": [
                {{"name": "æ™¯ç‚¹å", "type": "æ™¯ç‚¹ç±»å‹", "ticket": "é—¨ç¥¨ä»·æ ¼", "description": "ç®€çŸ­æè¿°"}}
            ]
        }}
    ],
    "tips": "æ—…è¡Œå°è´´å£«"
}}

ã€é‡è¦ã€‘
- åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºä»»ä½•Markdownè¯­æ³•
- ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥è¢«json.loads()è§£æ
- æ¯ä¸ªåŸå¸‚è‡³å°‘æ¨è2-4ä¸ªæ™¯ç‚¹"""

        return prompt

    def _parse_json_response(self, content: str) -> dict:
        """
        è§£æ LLM è¿”å›çš„ JSON å“åº”

        LLM æœ‰æ—¶ä¼šåœ¨ JSON å¤–é¢åŒ…è£¹ markdown ä»£ç å—æˆ–æ·»åŠ é¢å¤–æ–‡æœ¬ï¼Œ
        æ­¤å‡½æ•°è´Ÿè´£æå–çº¯ JSON å†…å®¹ã€‚

        Args:
            content: LLM è¿”å›çš„åŸå§‹å†…å®¹

        Returns:
            dict: è§£æåçš„ JSON å¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å› None
        """
        import re
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            return json.loads(content)
        except json.JSONDecodeError:
            pass

        # å°è¯•æå– JSON ä»£ç å—
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', content)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except:
                pass

        # å°è¯•æå–ä»»ä½• JSON å¯¹è±¡
        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            try:
                return json.loads(json_match.group())
            except:
                pass

        return None

    def _format_travel_response(self, data: dict) -> str:
        """
        æ ¼å¼åŒ–æ—…æ¸¸å“åº”

        å°† LLM ç”Ÿæˆçš„ JSON æ•°æ®æ ¼å¼åŒ–ä¸ºè§„èŒƒçš„ Markdown æ–‡æœ¬ã€‚

        Args:
            data: ç»“æ„åŒ–æ•°æ®å­—å…¸

        Returns:
            str: æ ¼å¼åŒ–åçš„ Markdown æ–‡æœ¬
        """
        lines = []

        # å¼€åœºç™½
        opening = data.get('opening', '')
        if opening:
            lines.append(opening)
            lines.append('')

        # åŸå¸‚æ¨è
        for i, city in enumerate(data.get('cities', [])):
            lines.append(f"## {city.get('emoji', '')} {city.get('name', '')}")
            lines.append('')

            # åŸå¸‚åŸºæœ¬ä¿¡æ¯
            lines.append(f"- **æ¨èå¤©æ•°**ï¼š{city.get('days', '3å¤©')}")
            lines.append(f"- **é¢„ç®—**ï¼šçº¦ **{city.get('budget', 'å¾…å®š')}/å¤©**")
            lines.append(f"- **æœ€ä½³æ—…è¡Œå­£èŠ‚**ï¼š{city.get('season', 'å››å­£çš†å®œ')}")
            lines.append('')

            # å¿…æ¸¸æ™¯ç‚¹
            lines.append('#### å¿…æ¸¸æ™¯ç‚¹ï¼š')
            attractions = city.get('attractions', [])
            for j, attr in enumerate(attractions, 1):
                ticket = attr.get('ticket', 'å…è´¹')
                ticket_str = f"é—¨ç¥¨ **{ticket}**" if ticket not in ['å…è´¹', '0', 0] else 'å®Œå…¨å…è´¹'
                lines.append(f"{j}. **{attr.get('name', 'æœªçŸ¥æ™¯ç‚¹')}**ï¼ˆ{attr.get('type', 'æ™¯ç‚¹')}ï¼‰- {ticket_str}")
                desc = attr.get('description', '')
                if desc:
                    lines.append(f"   - {desc}")
                lines.append('')

            # åŸå¸‚ä¹‹é—´åŠ ç©ºè¡Œ
            if i < len(data.get('cities', [])) - 1:
                lines.append('')

        # æ—…è¡Œå°è´´å£«
        tips = data.get('tips', '')
        if tips:
            lines.append('')
            lines.append('â˜€ï¸ æ—…è¡Œå°è´´å£«')
            lines.append('')
            lines.append(tips)

        return '\n'.join(lines)

    def get_conversation_history(self) -> list:
        """
        è·å–å¯¹è¯å†å²

        Returns:
            list: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        """
        return self.memory_manager.get_conversation_history()

    def clear_conversation(self) -> None:
        """
        æ¸…é™¤å¯¹è¯å†å²

        æ¸…ç©ºè®°å¿†ç®¡ç†å™¨å’Œ ReActAgent çš„çŠ¶æ€ï¼Œå‡†å¤‡æ¥å—æ–°ä¼šè¯ã€‚
        """
        self.memory_manager.clear_conversation()
        self.react_agent.reset()

    # ==========================================================================
    # å¤šæ¨¡å¼å¯¹è¯å¤„ç†
    # ==========================================================================

    async def process_with_mode(
        self,
        user_input: str,
        mode: ChatMode = ChatMode.REACT,
        answer_callback=None,
        done_callback=None,
        thinking_callback=None
    ) -> Dict[str, Any]:
        """
        æ ¹æ®æŒ‡å®šæ¨¡å¼å¤„ç†ç”¨æˆ·è¾“å…¥

        æ”¯æŒä¸‰ç§å¯¹è¯æ¨¡å¼ï¼š
        1. Direct Mode: ç›´æ¥è°ƒç”¨ LLMï¼Œå¿«é€Ÿå“åº”ç®€å•é—®é¢˜
        2. ReAct Mode: æ¨ç†ä¸è¡ŒåŠ¨äº¤æ›¿ï¼Œé€‚åˆéœ€è¦å·¥å…·è°ƒç”¨çš„åœºæ™¯
        3. Plan Mode: å…ˆè§„åˆ’åæ‰§è¡Œï¼Œé€‚åˆå¤æ‚ä»»åŠ¡

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            mode: å¯¹è¯æ¨¡å¼
            answer_callback: ç­”æ¡ˆå›è°ƒ
            done_callback: å®Œæˆå›è°ƒ
            thinking_callback: æ€è€ƒå›è°ƒ

        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        import logging
        import time as time_module
        logger = logging.getLogger(__name__)

        logger.info(f"[Agent] å¼€å§‹å¤„ç† (mode={mode.value}): {user_input[:50]}...")
        start_time = time_module.time()

        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²
        self.memory_manager.add_message('user', user_input)

        context = {
            'user_query': user_input,
            'user_preference': self.memory_manager.get_user_preference()
        }

        # æ ¹æ®æ¨¡å¼å¤„ç†
        if mode == ChatMode.DIRECT:
            result = await self._process_direct_mode(user_input, answer_callback, done_callback, thinking_callback)
        elif mode == ChatMode.PLAN:
            result = await self._process_plan_mode(user_input, context, answer_callback, done_callback, thinking_callback)
        else:
            # é»˜è®¤ä½¿ç”¨ ReAct æ¨¡å¼
            result = await self._process_react_mode(user_input, context, answer_callback, done_callback, thinking_callback)

        elapsed = time_module.time() - start_time
        logger.info(f"[Agent] å¤„ç†å®Œæˆ (mode={mode.value}), è€—æ—¶: {elapsed:.2f}ç§’")

        return result

    async def _process_direct_mode(
        self,
        user_input: str,
        answer_callback=None,
        done_callback=None,
        thinking_callback=None
    ) -> Dict[str, Any]:
        """
        ç›´æ¥è°ƒç”¨ LLM æ¨¡å¼

        ç‰¹ç‚¹ï¼š
        - å¿«é€Ÿå“åº”ï¼Œæ— å·¥å…·è°ƒç”¨
        - é€‚åˆç®€å•å¯¹è¯å’Œä¸€èˆ¬é—®é¢˜
        - ä¸å±•ç¤ºæ€è€ƒè¿‡ç¨‹
        """
        import logging
        import asyncio
        logger = logging.getLogger(__name__)

        # å‘é€æ€è€ƒå¼€å§‹
        if thinking_callback:
            thinking_callback("ã€ç›´æ¥æ¨¡å¼ã€‘ç›´æ¥ç”Ÿæˆå›ç­”...\n\n", 0.0)

        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": user_input}
        ]

        # æµå¼ç”Ÿæˆå›ç­”
        if hasattr(self.llm_client, 'chat_stream') and answer_callback:
            accumulated_answer = ""
            token_count = 0

            for token in self.llm_client.chat_stream(messages, temperature=0.7):
                token_count += 1
                accumulated_answer += token
                answer_callback(token)
                await asyncio.sleep(0.01)

            answer = accumulated_answer
            logger.info(f"[Agent] ç›´æ¥æ¨¡å¼å®Œæˆ, {token_count} tokens")
        else:
            # éæµå¼
            result = self.llm_client.chat(messages, temperature=0.7)
            answer = result.get('content', 'æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„æ„æ€ã€‚')

        # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
        self.memory_manager.add_message('assistant', answer)

        result = {
            "success": True,
            "answer": answer,
            "mode": "direct",
            "reasoning": {
                "text": "<thinking>\n[Direct Mode]\nç›´æ¥è°ƒç”¨ LLM ç”Ÿæˆå›ç­”\n</thinking>",
                "total_steps": 0,
                "tools_used": []
            },
            "history": []
        }

        # è°ƒç”¨å®Œæˆå›è°ƒ
        if done_callback:
            done_callback(result)

        return result

    async def _process_plan_mode(
        self,
        user_input: str,
        context: Dict,
        answer_callback=None,
        done_callback=None,
        thinking_callback=None
    ) -> Dict[str, Any]:
        """
        è§„åˆ’åæ‰§è¡Œæ¨¡å¼

        ç‰¹ç‚¹ï¼š
        1. å…ˆä½¿ç”¨ LLM ç”Ÿæˆå®Œæ•´çš„æ‰§è¡Œè®¡åˆ’
        2. å†é€æ­¥æ‰§è¡Œè®¡åˆ’ä¸­çš„æ­¥éª¤
        3. æœ€åç”Ÿæˆæœ€ç»ˆå›ç­”

        é€‚åˆå¤æ‚ä»»åŠ¡ï¼Œå¦‚å¤šæ—¥è¡Œç¨‹è§„åˆ’
        """
        import logging
        import asyncio
        import json as json_util
        logger = logging.getLogger(__name__)

        step_times = []

        # Step 1: ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        if thinking_callback:
            thinking_callback("ã€è§„åˆ’æ¨¡å¼ã€‘æ­£åœ¨ç”Ÿæˆæ‰§è¡Œè®¡åˆ’...\n\n", 0.0)

        plan_start = asyncio.get_event_loop()
        plan_prompt = f"""ç”¨æˆ·è¯·æ±‚: {user_input}

è¯·åˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ï¼Œä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
    "steps": [
        {{
            "step": 1,
            "action": "å·¥å…·åç§°",
            "params": {{"å‚æ•°": "å€¼"}},
            "description": "æ­¥éª¤æè¿°"
        }}
    ],
    "estimated_time": "é¢„è®¡æ€»æ—¶é—´"
}}"

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        plan_result = self.llm_client.chat([
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": plan_prompt}
        ], temperature=0.3)

        if not plan_result.get('success'):
            return {
                "success": False,
                "error": "è§„åˆ’ç”Ÿæˆå¤±è´¥",
                "mode": "plan"
            }

        plan_content = plan_result.get('content', '{}')
        try:
            # å°è¯•ç›´æ¥è§£æ JSON
            plan_data = json_util.loads(plan_content)
            logger.info(f"[Plan] ç›´æ¥è§£ææˆåŠŸ: {plan_data}")
        except json_util.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå– JSON
            logger.warning(f"[Plan] ç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–: {plan_content[:200]}...")
            plan_data = self._extract_json_from_plan(plan_content)
            logger.info(f"[Plan] æå–ç»“æœ: {plan_data}")

        steps = plan_data.get('steps', [])
        if not steps:
            logger.warning(f"[Plan] steps ä¸ºç©ºï¼ŒåŸå§‹å†…å®¹: {plan_content[:500]}...")
            # å°è¯•æ›´å®½æ¾çš„è§£æ
            if 'steps' in plan_content:
                import re
                # åŒ¹é…æ•´ä¸ª steps æ•°ç»„ä¸­çš„æ¯ä¸ªæ­¥éª¤å¯¹è±¡
                step_pattern = re.compile(r'\{\s*"action"\s*:\s*"([^"]+)"\s*,\s*"params"\s*:\s*(\{[^}]*\})\s*,\s*"description"\s*:\s*"([^"]+)"\s*\}')
                step_matches = step_pattern.findall(plan_content)

                if step_matches:
                    logger.info(f"[Plan] æ‰¾åˆ°æ­¥éª¤: {step_matches}")
                    steps = []
                    for action, params_str, description in step_matches:
                        try:
                            params = json_util.loads(params_str) if params_str else {}
                        except:
                            params = {}
                        steps.append({
                            "action": action,
                            "params": params,
                            "description": description
                        })
                else:
                    # å°è¯•åªæå– action
                    step_items = re.findall(r'"action"\s*:\s*"([^"]+)"', plan_content)
                    if step_items:
                        logger.info(f"[Plan] åªæ‰¾åˆ° action: {step_items}")
                        steps = [{"action": s, "params": {}, "description": s} for i, s in enumerate(step_items)]

        step_elapsed = (asyncio.get_event_loop().time() - plan_start.time()) if hasattr(plan_start, 'time') else 0
        step_times.append(("è§„åˆ’", step_elapsed))

        if thinking_callback:
            thinking_callback(f"ã€è§„åˆ’æ¨¡å¼ã€‘è®¡åˆ’ç”Ÿæˆå®Œæˆï¼Œå…± {len(steps)} ä¸ªæ­¥éª¤\n\n", step_elapsed)

        # Step 2: æ‰§è¡Œè®¡åˆ’
        history = []
        reasoning_text = "[è§„åˆ’æ¨¡å¼æ‰§è¡Œ]\n\n"

        for i, step in enumerate(steps):
            step_num = i + 1
            action_name = step.get('action', '')
            params = step.get('params', {})
            description = step.get('description', '')

            step_start = asyncio.get_event_loop()

            if thinking_callback:
                thinking_callback(f"ã€è§„åˆ’æ¨¡å¼ã€‘æ‰§è¡Œæ­¥éª¤ {step_num}/{len(steps)}: {description}\n\n", 0.0)

            reasoning_text += f"æ­¥éª¤ {step_num}: {description}\n"

            # æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
            result = {'success': False}
            if action_name and action_name != 'none':
                tool = self.react_agent.tool_registry.get_tool(action_name)
                if tool:
                    try:
                        result = await tool.execute(**params) if hasattr(tool, 'execute') else tool(params)
                        reasoning_text += f"  - æ‰§è¡Œ: {action_name}\n"
                        reasoning_text += f"  - ç»“æœ: {str(result)[:100]}...\n"
                    except Exception as e:
                        reasoning_text += f"  - é”™è¯¯: {str(e)}\n"
                        result = {'success': False, 'error': str(e)}

            step_elapsed = (asyncio.get_event_loop().time() - step_start.time()) if hasattr(step_start, 'time') else 0
            step_times.append((action_name, step_elapsed))

            history.append({
                'step': step_num,
                'action': action_name,
                'params': params,
                'result': result,
                'description': description
            })

        # Step 3: ç”Ÿæˆæœ€ç»ˆå›ç­”
        if thinking_callback:
            thinking_callback("ã€è§„åˆ’æ¨¡å¼ã€‘æ­£åœ¨ç”Ÿæˆæœ€ç»ˆå›ç­”...\n\n", 0.0)

        # æ”¶é›†å·¥å…·æ‰§è¡Œç»“æœ
        tool_results = [h.get('result', {}) for h in history if h.get('result', {}).get('success')]

        if tool_results:
            answer = self._generate_answer_from_results(user_input, tool_results)
        else:
            # ç›´æ¥ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
            final_prompt = f"""ç”¨æˆ·è¯·æ±‚: {user_input}

æ‰§è¡Œè®¡åˆ’å·²å®Œæˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›ç­”ï¼š
{json_util.dumps(history, ensure_ascii=False, indent=2)}

è¯·æä¾›è¯¦ç»†ã€ç»“æ„åŒ–çš„å›ç­”ã€‚"""
            final_result = self.llm_client.chat([
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": final_prompt}
            ], temperature=0.7)
            answer = final_result.get('content', 'æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ã€‚')

        # é€šè¿‡ answer_callback å‘é€æœ€ç»ˆå›ç­”ç»™å‰ç«¯
        if answer_callback:
            answer_callback(answer)

        self.memory_manager.add_message('assistant', answer)

        # æ„å»ºæ¨ç†æ–‡æœ¬
        reasoning_text += "\næ‰§è¡Œå®Œæˆã€‚"
        full_reasoning = f"""<thinking>
[è§„åˆ’æ¨¡å¼]
{reasoning_text}

[æ­¥éª¤è€—æ—¶]
{chr(10).join([f"- {name}: {t:.2f}ç§’" for name, t in step_times])}
</thinking>"""

        result = {
            "success": True,
            "answer": answer,
            "mode": "plan",
            "reasoning": {
                "text": full_reasoning,
                "total_steps": len(steps),
                "tools_used": [h.get('action') for h in history if h.get('action')]
            },
            "history": history,
            "plan": steps
        }

        # è°ƒç”¨å®Œæˆå›è°ƒ
        if done_callback:
            done_callback(result)

        return result

    def _extract_json_from_plan(self, content: str) -> Dict:
        """ä»è®¡åˆ’æ–‡æœ¬ä¸­æå– JSON"""
        import re
        import json as json_util
        json_match = re.search(r'\{[^{}]*\}', content)
        if json_match:
            try:
                return json_util.loads(json_match.group())
            except json_util.JSONDecodeError:
                pass
        return {}

    def _generate_answer_from_results(self, user_input: str, results: List[Dict]) -> str:
        """æ ¹æ®å·¥å…·æ‰§è¡Œç»“æœç”Ÿæˆå›ç­”"""
        import json
        prompt = f"""ç”¨æˆ·è¯·æ±‚: {user_input}

å·¥å…·æ‰§è¡Œç»“æœ:
{json.dumps(results, ensure_ascii=False, indent=2)}

è¯·æ ¹æ®ä»¥ä¸Šç»“æœï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„æ¸…æ™°ã€å†…å®¹ä¸°å¯Œçš„æ—…æ¸¸å›ç­”ã€‚"""
        result = self.llm_client.chat([
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ], temperature=0.7)
        return result.get('content', 'å¤„ç†å®Œæˆ')

    async def _process_react_mode(
        self,
        user_input: str,
        context: Dict,
        answer_callback=None,
        done_callback=None,
        thinking_callback=None
    ) -> Dict[str, Any]:
        """
        ReAct æ¨ç†æ¨¡å¼

        ç‰¹ç‚¹ï¼š
        - æ€è€ƒ â†’ è¡ŒåŠ¨ â†’ è§‚å¯Ÿ â†’ è¯„ä¼°å¾ªç¯
        - æ”¯æŒåŠ¨æ€å·¥å…·è°ƒç”¨
        - å±•ç¤ºå®Œæ•´çš„æ¨ç†è¿‡ç¨‹
        """
        import logging
        import asyncio
        import time as time_module
        logger = logging.getLogger(__name__)

        # è®¾ç½®æ€è€ƒæµå¼å›è°ƒ
        if hasattr(self.react_agent, 'set_think_stream_callback') and thinking_callback:
            self.react_agent.set_think_stream_callback(thinking_callback)

        # æ‰§è¡Œ ReAct å¾ªç¯
        result = await self.react_agent.run(user_input, context)
        logger.info(f"[Agent] ReAct æ‰§è¡Œå®Œæˆ, success={result.get('success')}")

        if result.get('success'):
            history = result.get('history', [])
            reasoning_text = self._build_reasoning_text(history)
            answer = self._extract_answer(history)

            self.memory_manager.add_message('assistant', answer)

            # æ„å»º LLM æ¶ˆæ¯ç”Ÿæˆæœ€ç»ˆå›ç­”
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œæä¾›è¯¦ç»†ã€å‡†ç¡®çš„æ—…æ¸¸å»ºè®®å’Œè§„åˆ’ã€‚"
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ]

            # æµå¼ç”Ÿæˆæœ€ç»ˆå›ç­”
            if hasattr(self.llm_client, 'chat_stream') and answer_callback:
                token_count = 0
                accumulated_answer = ""

                for token in self.llm_client.chat_stream(messages, temperature=0.7):
                    token_count += 1
                    accumulated_answer += token
                    answer_callback(token)
                    await asyncio.sleep(0.01)

                answer = accumulated_answer
                logger.info(f"[Agent] ReAct æµå¼ç”Ÿæˆå®Œæˆ, {token_count} tokens")

            return {
                "success": True,
                "answer": answer,
                "mode": "react",
                "reasoning": {
                    "text": reasoning_text,
                    "total_steps": len(history),
                    "tools_used": self._extract_tools_used(history)
                },
                "history": history
            }
        else:
            return {
                "success": False,
                "error": result.get('error', 'å¤„ç†å¤±è´¥'),
                "mode": "react",
                "reasoning": None,
                "history": result.get('history', [])
            }
